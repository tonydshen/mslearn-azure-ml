{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Check azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1666789326586
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: azure-ai-ml\n",
            "Version: 1.29.0\n",
            "Summary: Microsoft Azure Machine Learning Client Library for Python\n",
            "Home-page: https://github.com/Azure/azure-sdk-for-python\n",
            "Author: Microsoft Corporation\n",
            "Author-email: azuresdkengsysadmins@microsoft.com\n",
            "License: MIT License\n",
            "Location: c:\\Users\\Tony\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\n",
            "Requires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\n",
            "Required-by: \n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip show azure-ai-ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Connect to my workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1666789343369
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MLClient(credential=<azure.identity._credentials.default.DefaultAzureCredential object at 0x0000024326455FD0>,\n",
              "         subscription_id=c6cdfd9c-3767-4be7-a343-0acd27ddefb9,\n",
              "         resource_group_name=rg-tshen-0917,\n",
              "         workspace_name=ml-ws-tshen-0917)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "#Enter    details    of    your    Azure    Machine    Learning    workspace\n",
        "subscription_id = 'c6cdfd9c-3767-4be7-a343-0acd27ddefb9'\n",
        "resource_group = 'rg-tshen-0917'\n",
        "workspace = 'ml-ws-tshen-0917'\n",
        "#connect to the workspace\n",
        "ml_client    =    MLClient(DefaultAzureCredential(),    subscription_id,\n",
        "resource_group,    workspace)\n",
        "ml_client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write local-fix-missing-data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./src/local-fix-missing-data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./src/local-fix-missing-data.py\n",
        "# import libraries\n",
        "import argparse\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "\n",
        "# get parameters\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_data\", type=str, help='Path to input data')\n",
        "parser.add_argument('--output_data', type=str, help='Path of output data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# load the data (passed as an input dataset)\n",
        "# data_path = args.input_data\n",
        "# all_files = glob.glob(data_path + \"/*.csv\")\n",
        "# df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "df = pd.read_csv(args.input_data)\n",
        "\n",
        "# log row count input data\n",
        "row_count = (len(df))\n",
        "mlflow.log_metric('row count input data', row_count)\n",
        "\n",
        "# remove nulls\n",
        "df = df.dropna()\n",
        "\n",
        "# log processed rows\n",
        "row_count_processed = (len(df))\n",
        "mlflow.log_metric('row count output data', row_count_processed)\n",
        "\n",
        "# set the processed data as output\n",
        "output_df = df.to_csv(args.output_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1666790805418
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!python ./src/local-fix-missing-data.py --input_data ./data/diabetes.csv --output_data ./data/diabetes-out.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write local-normalize-data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./src/local-normalize-data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./src/local-normalize-data.py\n",
        "# import libraries\n",
        "import argparse\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# get parameters\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input_data\", type=str, help='Path to input data')\n",
        "parser.add_argument('--output_data', type=str, help='Path of output data')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\"\"\"\n",
        "# load the data (passed as an input dataset)\n",
        "print(\"files in input_data path: \")\n",
        "arr = os.listdir(args.input_data)\n",
        "print(arr)\n",
        "\n",
        "for filename in arr:\n",
        "    print(\"reading file: %s ...\" % filename)\n",
        "    with open(os.path.join(args.input_data, filename), \"r\") as handle:\n",
        "        print(handle.read())\n",
        "\n",
        "data_path = args.input_data\n",
        "all_files = glob.glob(data_path + \"/*.csv\")\n",
        "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(args.input_data)\n",
        "# log row count input data\n",
        "row_count = (len(df))\n",
        "mlflow.log_metric('row count input data', row_count)\n",
        "\n",
        "# remove nulls\n",
        "df = df.dropna()\n",
        "\n",
        "# log processed rows\n",
        "row_count_processed = (len(df))\n",
        "mlflow.log_metric('row count output data', row_count_processed)\n",
        "\n",
        "# set the processed data as output\n",
        "output_df = df.to_csv(args.output_data)\n",
        "\n",
        "# normalize the numeric columns\n",
        "scaler = MinMaxScaler()\n",
        "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# log processed rows\n",
        "row_count_processed = (len(df))\n",
        "mlflow.log_metric('row count output data', row_count_processed)\n",
        "\n",
        "# set the processed data as output\n",
        "output_df = df.to_csv(args.output_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run local-normalize-data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python ./src/local-normalize-data.py --input_data ./data/diabetes.csv --output_data ./data/diabetes-out-2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write local-train-decision-tree.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./src/local-train-decision-tree.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./src/local-train-decision-tree.py\n",
        "# Import libraries\n",
        "import argparse\n",
        "import glob\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# get parameters\n",
        "parser = argparse.ArgumentParser(\"train\")\n",
        "parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
        "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "training_data = args.training_data\n",
        "model_output = args.model_output\n",
        "\n",
        "\"\"\"\n",
        "# load the prepared data file in the training folder\n",
        "print(\"Loading Data...\")\n",
        "data_path = args.training_data\n",
        "all_files = glob.glob(data_path + \"/*.csv\")\n",
        "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_csv(args.training_data)\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = (\n",
        "    df[\n",
        "        [\n",
        "            \"Pregnancies\",\n",
        "            \"PlasmaGlucose\",\n",
        "            \"DiastolicBloodPressure\",\n",
        "            \"TricepsThickness\",\n",
        "            \"SerumInsulin\",\n",
        "            \"BMI\",\n",
        "            \"DiabetesPedigree\",\n",
        "            \"Age\",\n",
        "        ]\n",
        "    ].values,\n",
        "    df[\"Diabetic\"].values,\n",
        ")\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=0\n",
        ")\n",
        "\n",
        "# Train a decision tree model\n",
        "print('Training a decision tree model...')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "acc = np.average(y_pred == y_test)\n",
        "print(\"Accuracy:\", acc)\n",
        "mlflow.log_metric(\"Accuracy\", float(acc))\n",
        "\n",
        "# Calculate AUC\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "print(\"AUC: \" + str(auc))\n",
        "mlflow.log_metric(\"AUC\", float(auc))\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "# Plot the diagonal 50% line\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "# Plot the FPR and TPR achieved by our model\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.savefig(\"ROCcurve.png\")\n",
        "mlflow.log_artifact(\"ROCcurve.png\")\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(\n",
        "            x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\",\n",
        "            size=\"xx-large\"\n",
        "        )\n",
        "\n",
        "plt.xlabel(\"Predictions\", fontsize=18)\n",
        "plt.ylabel(\"Actuals\", fontsize=18)\n",
        "plt.title(\"Confusion Matrix\", fontsize=18)\n",
        "plt.savefig(\"ConfusionMatrix.png\")\n",
        "mlflow.log_artifact(\"ConfusionMatrix.png\")\n",
        "\n",
        "# Output the model and test data\n",
        "# Output the model and test data\n",
        "# pickle.dump(model, open((Path(args.model_output) / \"model.sav\"), \"wb\"))\n",
        "pickle.dump(model, open(model_output, \"wb\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run local-train-decision-tree.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training a decision tree model...\n",
            "Accuracy: 0.8846666666666667\n",
            "AUC: 0.8711328921836907\n"
          ]
        }
      ],
      "source": [
        "!python ./src/local-train-decision-tree.py --training_data ./data/diabetes-out-2.csv --model_output ./model/decision-tree.save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Write local-train-logistic-regression.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./src/local-train-logistic-regression.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./src/local-train-logistic-regression.py\n",
        "# Import libraries\n",
        "import argparse\n",
        "import glob\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# get parameters\n",
        "parser = argparse.ArgumentParser(\"train\")\n",
        "parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
        "parser.add_argument(\"--reg_rate\", type=float, default=0.01)\n",
        "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
        "parser.add_argument(\"--test_size\", type=float, default=0.30, help=\"test size\")\n",
        "parser.add_argument(\"--random_state\", type=int, default=None, help=\"random state\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "print(args)\n",
        "\n",
        "training_data = args.training_data\n",
        "model_output = args.model_output\n",
        "reg_rate = args.reg_rate\n",
        "test_size = args.test_size\n",
        "random_state = args.random_state\n",
        "\n",
        "# load the prepared data file in the training folder\n",
        "print(\"Loading Data...\")\n",
        "\n",
        "# data_path = args.training_data\n",
        "# all_files = glob.glob(data_path + \"/*.csv\")\n",
        "# df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
        "df = pd.read_csv(training_data)\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = (\n",
        "    df[\n",
        "        [\n",
        "            \"Pregnancies\",\n",
        "            \"PlasmaGlucose\",\n",
        "            \"DiastolicBloodPressure\",\n",
        "            \"TricepsThickness\",\n",
        "            \"SerumInsulin\",\n",
        "            \"BMI\",\n",
        "            \"DiabetesPedigree\",\n",
        "            \"Age\",\n",
        "        ]\n",
        "    ].values,\n",
        "    df[\"Diabetic\"].values,\n",
        ")\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=random_state\n",
        ")\n",
        "\n",
        "# Train a logistic regression model\n",
        "print('Training a logistic regression model...')\n",
        "model = LogisticRegression(C=1 / reg_rate, solver=\"liblinear\").fit(\n",
        "    X_train, y_train\n",
        ")\n",
        "\n",
        "# Calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "acc = np.average(y_pred == y_test)\n",
        "print(\"Accuracy:\", acc)\n",
        "mlflow.log_metric(\"Accuracy\", float(acc))\n",
        "\n",
        "# Calculate AUC\n",
        "y_pred_proba = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "print(\"AUC: \" + str(auc))\n",
        "mlflow.log_metric(\"AUC\", float(auc))\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "# Plot the diagonal 50% line\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "# Plot the FPR and TPR achieved by our model\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.savefig(\"ROCcurve.png\")\n",
        "mlflow.log_artifact(\"ROCcurve.png\")\n",
        "\n",
        "# Create confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
        "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        ax.text(\n",
        "            x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\",\n",
        "            size=\"xx-large\"\n",
        "        )\n",
        "\n",
        "plt.xlabel(\"Predictions\", fontsize=18)\n",
        "plt.ylabel(\"Actuals\", fontsize=18)\n",
        "plt.title(\"Confusion Matrix\", fontsize=18)\n",
        "plt.savefig(\"ConfusionMatrix.png\")\n",
        "mlflow.log_artifact(\"ConfusionMatrix.png\")\n",
        "\n",
        "# Output the model and test data\n",
        "# pickle.dump(model, open((Path(args.model_output) / \"model.sav\"), \"wb\"))\n",
        "pickle.dump(model, open(model_output, \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run local-train-logistic-regression.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(training_data='./data/diabetes-out-2.csv', reg_rate=0.01, model_output='./model/log-regression-tree.save', test_size=0.3, random_state=None)\n",
            "Loading Data...\n",
            "Training a logistic regression model...\n",
            "Accuracy: 0.7813333333333333\n",
            "AUC: 0.8548116214965853\n"
          ]
        }
      ],
      "source": [
        "!python ./src/local-train-logistic-regression.py --training_data ./data/diabetes-out-2.csv --model_output ./model/log-regression-tree.save"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
