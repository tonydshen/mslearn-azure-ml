{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d919dca6",
   "metadata": {},
   "source": [
    "Lab 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8009d382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azure-ai-ml\n",
      "Version: 1.29.0\n",
      "Summary: Microsoft Azure Machine Learning Client Library for Python\n",
      "Home-page: https://github.com/Azure/azure-sdk-for-python\n",
      "Author: Microsoft Corporation\n",
      "Author-email: azuresdkengsysadmins@microsoft.com\n",
      "License: MIT License\n",
      "Location: /home/tshen/dp-100/mslearn-azure-ml/.venv/lib/python3.12/site-packages\n",
      "Requires: azure-common, azure-core, azure-mgmt-core, azure-monitor-opentelemetry, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c40abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=http://localhost:5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://localhost:5000'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - Set MLflow enviroment varibale\n",
    "%env MLFLOW_TRACKING_URI=http://localhost:5000\n",
    "# verify it is set\n",
    "%env MLFLOW_TRACKING_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3dedd1",
   "metadata": {},
   "source": [
    "Credit card model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86368427",
   "metadata": {},
   "source": [
    "Step 1 - Download data and create data and script directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ac2bf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/credit_card.csv', <http.client.HTTPMessage at 0x720494257470>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 - download the data and create data folder and script folder if not exist\n",
    "# download data locally\n",
    "import os\n",
    "import urllib.request\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "os.makedirs(\"./src\", exist_ok=True)\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://azuremlexamples.blob.core.windows.net/datasets/credit_card/default_of_credit_card_clients.csv\",\n",
    "    \"./data/credit_card.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ef6a8",
   "metadata": {},
   "source": [
    "Step 2 - Check and start MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65d38de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/24 10:44:39 INFO mlflow.tracking.fluent: Experiment with name 'credit_card' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/468553786695061552', creation_time=1758728679644, experiment_id='468553786695061552', last_update_time=1758728679644, lifecycle_stage='active', name='credit_card', tags={}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - Check mlflow and start experiment\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"credit_card\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94e7e9",
   "metadata": {},
   "source": [
    "Step 3 - Create credit-card-model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf332a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/credit-card-model.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile ./src/credit-card-model.py\n",
    "# Step 3 - Create a training script\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, default=0.25)\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"Model name\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    mlflow.start_run()\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    credit_df = pd.read_csv(args.data, header=1, index_col=0)\n",
    "    train_df, test_df = train_test_split(credit_df, test_size=args.test_train_ratio)\n",
    "\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "    X_train = train_df.values\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "    X_test = test_df.values\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=args.n_estimators, learning_rate=args.learning_rate)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    mlflow.sklearn.log_model(sk_model=clf, registered_model_name=args.registered_model_name, artifact_path=args.registered_model_name)\n",
    "    \n",
    "    # mlflow.sklearn.save_model(sk_model=clf, path=os.path.join(args.registered_model_name, \"trained_model\"))\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa56d1d",
   "metadata": {},
   "source": [
    "Step 4 - run credit card model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd867428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025/09/24 10:45:03 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89      5775\n",
      "           1       0.70      0.37      0.48      1725\n",
      "\n",
      "    accuracy                           0.82      7500\n",
      "   macro avg       0.77      0.66      0.69      7500\n",
      "weighted avg       0.80      0.82      0.80      7500\n",
      "\n",
      "2025/09/24 10:45:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "\u001b[31m2025/09/24 10:45:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Successfully registered model 'credit_card_model'.\n",
      "2025/09/24 10:45:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: credit_card_model, version 1\n",
      "Created version '1' of model 'credit_card_model'.\n",
      "ðŸƒ View run bemused-seal-212 at: http://localhost:5000/#/experiments/468553786695061552/runs/2089a56ccf7d43689f794602119397f7\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/468553786695061552\n"
     ]
    }
   ],
   "source": [
    "# Credit card model\n",
    "!python ./src/credit-card-model.py --data ./data/credit_card.csv --n_estimators 50 --learning_rate 0.2 --registered_model_name credit_card_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca3e85",
   "metadata": {},
   "source": [
    "Diabetes model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54c7eb",
   "metadata": {},
   "source": [
    "Step 1 - Fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b284ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/diabetes-fix-missing-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/diabetes-fix-missing-data.py\n",
    "# import libraries\n",
    "import argparse\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help='Path to input data')\n",
    "parser.add_argument('--output_data', type=str, help='Path of output data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# load the data (passed as an input dataset)\n",
    "# data_path = args.input_data\n",
    "# all_files = glob.glob(data_path + \"/*.csv\")\n",
    "# df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "df = pd.read_csv(args.input_data)\n",
    "\n",
    "# log row count input data\n",
    "row_count = (len(df))\n",
    "mlflow.log_metric('row count input data', row_count)\n",
    "\n",
    "# remove nulls\n",
    "df = df.dropna()\n",
    "\n",
    "# log processed rows\n",
    "row_count_processed = (len(df))\n",
    "mlflow.log_metric('row count output data', row_count_processed)\n",
    "\n",
    "# set the processed data as output\n",
    "output_df = df.to_csv(args.output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e14d0",
   "metadata": {},
   "source": [
    "Step 2 - Run data fix script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-fix-missing-data.py --input_data ./data/diabetes.csv --output_data ./data/diabetes-out.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa96203",
   "metadata": {},
   "source": [
    "Step 3 - Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf6f8ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/diabetes-normalize-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/diabetes-normalize-data.py\n",
    "# import libraries\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str, help='Path to input data')\n",
    "parser.add_argument('--output_data', type=str, help='Path of output data')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\"\"\"\n",
    "# load the data (passed as an input dataset)\n",
    "print(\"files in input_data path: \")\n",
    "arr = os.listdir(args.input_data)\n",
    "print(arr)\n",
    "\n",
    "for filename in arr:\n",
    "    print(\"reading file: %s ...\" % filename)\n",
    "    with open(os.path.join(args.input_data, filename), \"r\") as handle:\n",
    "        print(handle.read())\n",
    "\n",
    "data_path = args.input_data\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "# log row count input data\n",
    "row_count = (len(df))\n",
    "mlflow.log_metric('row count input data', row_count)\n",
    "\n",
    "# remove nulls\n",
    "df = df.dropna()\n",
    "\n",
    "# log processed rows\n",
    "row_count_processed = (len(df))\n",
    "mlflow.log_metric('row count output data', row_count_processed)\n",
    "\n",
    "# set the processed data as output\n",
    "output_df = df.to_csv(args.output_data)\n",
    "\n",
    "# normalize the numeric columns\n",
    "scaler = MinMaxScaler()\n",
    "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# log processed rows\n",
    "row_count_processed = (len(df))\n",
    "mlflow.log_metric('row count output data', row_count_processed)\n",
    "\n",
    "# set the processed data as output\n",
    "output_df = df.to_csv(args.output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3587b5",
   "metadata": {},
   "source": [
    "Step 4 - Run data normalization script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a6aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-normalize-data.py --input_data ./data/diabetes.csv --output_data ./data/diabetes-out-2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff069189",
   "metadata": {},
   "source": [
    "Step 5 - Create diabetes model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9080bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/diabetes-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/diabetes-model.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# load the diabetes dataset\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data\", type=str, help=\"Path to input data\")\n",
    "parser.add_argument(\"--test_size\", type=float, default=0.30, help=\"Proportion of the dataset to include in the test split\")\n",
    "parser.add_argument(\"--reg\", type=float, default=0.01, help=\"Regularization rate parameter\")\n",
    "parser.add_argument(\"--registered_model_name\", type=str, help=\"Model name\")\n",
    " \n",
    "args = parser.parse_args()\n",
    "args\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "diabetes = pd.read_csv(args.data)\n",
    "\n",
    "# Start an MLflow run\n",
    "mlflow.start_run()\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=args.test_size)\n",
    "\n",
    "# set regularization hyperparameter\n",
    "reg = args.reg\n",
    "\n",
    "# train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg, 'test size of', args.test_size)\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "\n",
    "# mlflow.sklearn.log_model(sk_model=model, registered_model_name=args.registered_model_name, artifact_path=args.registered_model_name)\n",
    "mlflow.sklearn.log_model(sk_model=model, registered_model_name=args.registered_model_name, name=args.registered_model_name)\n",
    "\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab05be3",
   "metadata": {},
   "source": [
    "Step 6 - Run diabetes-model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c1cf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Training a logistic regression model with regularization rate of 0.02 test size of 0.2\n",
      "Accuracy: 0.786\n",
      "AUC: 0.8607077822102089\n",
      "\u001b[31m2025/09/24 11:20:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'diabetes_model' already exists. Creating a new version of this model...\n",
      "2025/09/24 11:20:53 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: diabetes_model, version 2\n",
      "Created version '2' of model 'diabetes_model'.\n",
      "ðŸƒ View run stately-squid-613 at: http://localhost:5000/#/experiments/468553786695061552/runs/a16011736b144852bc1e27a221fbf4ff\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/468553786695061552\n"
     ]
    }
   ],
   "source": [
    "!python ./src/diabetes-model.py --data ./data/diabetes.csv --test_size 0.20 --reg 0.02 --registered_model_name diabetes_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d766e31",
   "metadata": {},
   "source": [
    "Step 7 - Create diabetes decision tree model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc074a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/diabetes-decision-tree-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/diabetes-decision-tree-model.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "training_data = args.training_data\n",
    "model_output = args.model_output\n",
    "\n",
    "\"\"\"\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "data_path = args.training_data\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(args.training_data)\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = (\n",
    "    df[\n",
    "        [\n",
    "            \"Pregnancies\",\n",
    "            \"PlasmaGlucose\",\n",
    "            \"DiastolicBloodPressure\",\n",
    "            \"TricepsThickness\",\n",
    "            \"SerumInsulin\",\n",
    "            \"BMI\",\n",
    "            \"DiabetesPedigree\",\n",
    "            \"Age\",\n",
    "        ]\n",
    "    ].values,\n",
    "    df[\"Diabetic\"].values,\n",
    ")\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=0\n",
    ")\n",
    "\n",
    "# Train a decision tree model\n",
    "print('Training a decision tree model...')\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "acc = np.average(y_pred == y_test)\n",
    "print(\"Accuracy:\", acc)\n",
    "mlflow.log_metric(\"Accuracy\", float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(\"AUC: \" + str(auc))\n",
    "mlflow.log_metric(\"AUC\", float(auc))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.savefig(\"ROCcurve.png\")\n",
    "mlflow.log_artifact(\"ROCcurve.png\")\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(\n",
    "            x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\",\n",
    "            size=\"xx-large\"\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Predictions\", fontsize=18)\n",
    "plt.ylabel(\"Actuals\", fontsize=18)\n",
    "plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "plt.savefig(\"ConfusionMatrix.png\")\n",
    "mlflow.log_artifact(\"ConfusionMatrix.png\")\n",
    "\n",
    "# Output the model and test data\n",
    "# Output the model and test data\n",
    "# pickle.dump(model, open((Path(args.model_output) / \"model.sav\"), \"wb\"))\n",
    "pickle.dump(model, open(model_output, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a77721",
   "metadata": {},
   "source": [
    "Step 8 - Run diabetes decision tree model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528367ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-decision-tree.py --training_data ./data/diabetes-out-2.csv --model_output ./model/decision-tree.save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440d838",
   "metadata": {},
   "source": [
    "Step 9 - Create diabetes Logistic Regression model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb2067af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/diabetes-logistic-regression-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/diabetes-logistic-regression-model.py\n",
    "# Import libraries\n",
    "import argparse\n",
    "import glob\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "parser.add_argument(\"--reg_rate\", type=float, default=0.01)\n",
    "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "training_data = args.training_data\n",
    "\n",
    "# load the prepared data file in the training folder\n",
    "print(\"Loading Data...\")\n",
    "data_path = args.training_data\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = (\n",
    "    df[\n",
    "        [\n",
    "            \"Pregnancies\",\n",
    "            \"PlasmaGlucose\",\n",
    "            \"DiastolicBloodPressure\",\n",
    "            \"TricepsThickness\",\n",
    "            \"SerumInsulin\",\n",
    "            \"BMI\",\n",
    "            \"DiabetesPedigree\",\n",
    "            \"Age\",\n",
    "        ]\n",
    "    ].values,\n",
    "    df[\"Diabetic\"].values,\n",
    ")\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=0\n",
    ")\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model...')\n",
    "model = LogisticRegression(C=1 / args.reg_rate, solver=\"liblinear\").fit(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "# Calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "acc = np.average(y_pred == y_test)\n",
    "print(\"Accuracy:\", acc)\n",
    "mlflow.log_metric(\"Accuracy\", np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "print(\"AUC: \" + str(auc))\n",
    "mlflow.log_metric(\"AUC\", np.float(auc))\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:, 1])\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.savefig(\"ROCcurve.png\")\n",
    "mlflow.log_artifact(\"ROCcurve.png\")\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(\n",
    "            x=j, y=i, s=conf_matrix[i, j], va=\"center\", ha=\"center\",\n",
    "            size=\"xx-large\"\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Predictions\", fontsize=18)\n",
    "plt.ylabel(\"Actuals\", fontsize=18)\n",
    "plt.title(\"Confusion Matrix\", fontsize=18)\n",
    "plt.savefig(\"ConfusionMatrix.png\")\n",
    "mlflow.log_artifact(\"ConfusionMatrix.png\")\n",
    "\n",
    "# Output the model and test data\n",
    "pickle.dump(model, open((Path(args.model_output) / \"model.sav\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92e947",
   "metadata": {},
   "source": [
    "Step 10 - Run diabetes logistic regression model script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-logistic-regression.py --training_data ./data/diabetes-out-2.csv --model_output ./model/log-regression-tree.save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f52756f",
   "metadata": {},
   "source": [
    "Step 11 - Create diabetes Random Forrest model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a383cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/diabetes-random-forrest-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/diabetes-random-forrest-model.py\n",
    "# train.py\n",
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--training_data\", type=str, help=\"Path to training data\")\n",
    "parser.add_argument(\"--n_estimators\", type=int, default=100, help=\"Number of trees in the forest\")\n",
    "parser.add_argument(\"--model_output\", type=str, help=\"Path of output model\")\n",
    "parser.add_argument(\"--test_size\", type=float, default=0.30, help=\"test size\")\n",
    "parser.add_argument(\"--random_state\", type=int, default=None, help=\"random state\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "training_data = args.training_data\n",
    "model_output = args.model_output\n",
    "n_estimators = args.n_estimators\n",
    "test_size = args.test_size\n",
    "random_state = args.random_state\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(training_data)  # Replace with your actual file\n",
    "\n",
    "# Assume the last column is the target\n",
    "# X = data.iloc[:, :-1]  # Features\n",
    "# y = data.iloc[:, -1]   # Target\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = (\n",
    "    df[\n",
    "        [\n",
    "            \"Pregnancies\",\n",
    "            \"PlasmaGlucose\",\n",
    "            \"DiastolicBloodPressure\",\n",
    "            \"TricepsThickness\",\n",
    "            \"SerumInsulin\",\n",
    "            \"BMI\",\n",
    "            \"DiabetesPedigree\",\n",
    "            \"Age\",\n",
    "        ]\n",
    "    ].values,\n",
    "    df[\"Diabetic\"].values,\n",
    ")\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "# Initialize and train the classifier\n",
    "model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab858bd4",
   "metadata": {},
   "source": [
    "Step 12 - Run diabetes Random Forrest model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-random-forrest-model.py --training_data ./diabetes-data/diabetes.csv --model_output ./model/random-forest.save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19041f",
   "metadata": {},
   "source": [
    "Step 13 - Track diabetes Logistic Regression model training with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b799cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "experiment_name = \"mlflow-experiment-diabetes\"\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34956136",
   "metadata": {},
   "source": [
    "Step 13a - Track diabetes Logistic Regression model training with autolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87732338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "import mlflow\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Reading data...\")\n",
    "df = pd.read_csv('./data/diabetes.csv')\n",
    "df.head()\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    # Train with GPU if available\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c397c",
   "metadata": {},
   "source": [
    "Step 13b - Track diabetes Logistic Regression model training w/o autolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61451f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46702ad",
   "metadata": {},
   "source": [
    "Step 13c - Track diabetes Tensorflow Framework LR model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abd86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # with tf.device('/GPU:0'):\n",
    "    model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    mlflow.log_param(\"regularization_rate\", 0.1)\n",
    "    mlflow.log_metric(\"Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa7c87",
   "metadata": {},
   "source": [
    "Step 13d - Track diabetes sklearn LR model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f668b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run():\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = LogisticRegression(C=1/0.1, solver=\"liblinear\").fit(X_train, y_train)\n",
    "        y_hat = model.predict(X_test)\n",
    "        acc = np.average(y_hat == y_test)\n",
    "        mlflow.log_param(\"regularization_rate\", 0.1)\n",
    "        mlflow.log_metric(\"Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccb5da",
   "metadata": {},
   "source": [
    "Step 13e - Track diabetes sklearn Decision Tree model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec5721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "\n",
    "    mlflow.log_param(\"estimator\", \"DecisionTreeClassifier\")\n",
    "    mlflow.log_metric(\"Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004275c",
   "metadata": {},
   "source": [
    "Step 13f - Track diabetes sklearn DT model training and log artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94097116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "\n",
    "    # plot ROC curve\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "\n",
    "    mlflow.log_param(\"estimator\", \"DecisionTreeClassifier\")\n",
    "    mlflow.log_metric(\"Accuracy\", acc)\n",
    "    mlflow.log_artifact(\"ROC-Curve.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d0c12",
   "metadata": {},
   "source": [
    "Step 13g - Create diabetes MLflow tracking custom log script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c132ca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/diabetes-model-mlflow-custom-log.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/diabetes-model-mlflow-custom-log.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # evaluate model\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that splits the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    'SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function that trains the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    print(\"Training model...\")\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "# function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "    mlflow.log_metric(\"Accuracy\", acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "    mlflow.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\")\n",
    "    mlflow.log_artifact(\"ROC-Curve.png\")    \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7147b96",
   "metadata": {},
   "source": [
    "Step 13h - Run diabetes MLflow tracking custom log script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-model-mlflow-custom-log.py --training_data ./data/diabetes.csv --reg_rate 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb42fb8",
   "metadata": {},
   "source": [
    "Step 13i - Create diabetes MLflow tracking autolog script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff38ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/diabetes-model-mlflow-autolog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/diabetes-model-mlflow-autolog.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # enable autologging\n",
    "    mlflow.autolog()\n",
    "\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    print(\"Reading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that splits the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X, y = df[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness',\n",
    "    'SerumInsulin','BMI','DiabetesPedigree','Age']].values, df['Diabetic'].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function that trains the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    print(\"Training model...\")\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "# function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\") \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48311fe2",
   "metadata": {},
   "source": [
    "Step 13j - Run diabetes MLflow tracking autolog script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c84d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/diabetes-model-mlflow-autolog-log.py --training_data ./data/diabetes.csv --reg_rate 0.01 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760dd6c7",
   "metadata": {},
   "source": [
    "Step 13k - Query diabetes MLflow logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dab5ccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3675 /home/tshen/dp-100/mslearn-azure-ml/.venv/bin/python3 /home/tshen/dp-100/mslearn-azure-ml/.venv/bin/mlflow ui\n",
      "26709 /home/tshen/dp-100/mslearn-azure-ml/.venv/bin/python3 /home/tshen/dp-100/mslearn-azure-ml/.venv/bin/mlflow models serve -m models:/wine-quality-predictor/1 --port 5002 --env-manager local\n",
      "credit_card\n",
      "Experiment with 5000\n",
      "/my-experiment\n",
      "MLflow Quickstart\n",
      "wine-quality-optimization\n",
      "Default\n",
      "None\n",
      "Experiment 'diabetes-training' not found. Please check the experiment name or create it first.\n"
     ]
    }
   ],
   "source": [
    "# check if mlflow server is running\n",
    "!pgrep -a mlflow\n",
    "\n",
    "import mlflow\n",
    "experiments = mlflow.search_experiments()\n",
    "for exp in experiments:\n",
    "    print(exp.name)\n",
    "    \n",
    "experiment_name = \"diabetes-training\"\n",
    "exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(exp)\n",
    "\n",
    "if exp is not None:\n",
    "    mlflow.search_runs(exp.experiment_id)\n",
    "    mlflow.search_runs(exp.experiment_id, order_by=[\"start_time DESC\"], max_results=2)\n",
    "\n",
    "    query = \"metrics.AUC > 0.8 and tags.model_type = 'LogisticRegression'\"\n",
    "    mlflow.search_runs(exp.experiment_id, filter_string=query)\n",
    "else:\n",
    "    print(f\"Experiment '{experiment_name}' not found. Please check the experiment name or create it first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7508f",
   "metadata": {},
   "source": [
    "Step 14 - Create PyTorch Framework IrisClassifier model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af6b34ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/pytorch_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/pytorch_iris.py\n",
    "# pytorch_iris.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--training_data\", type=str, default='./iris-data/iris.csv', help=\"Path to training data\")\n",
    "parser.add_argument(\"--reg_rate\", type=float, default=0.01)\n",
    "parser.add_argument(\"--model_output\", type=str, default='./model/iris.save', help=\"Path of output model\")\n",
    "parser.add_argument(\"--test_size\", type=float, default=0.30, help=\"test size\")\n",
    "parser.add_argument(\"--random_state\", type=int, default=0, help=\"random state\")\n",
    "parser.add_argument(\"--n_epoch\", type=int, default=50, help=\"number of epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"learning rate\")\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "training_data = args.training_data\n",
    "reg_rate = args.reg_rate\n",
    "model_output = args.model_output\n",
    "test_size = args.test_size\n",
    "random_state = args.random_state\n",
    "n_epoch = args.n_epoch\n",
    "learning_rate = args.learning_rate\n",
    "\n",
    "# Load Iris dataset\n",
    "df = pd.read_csv(training_data)  # Replace with your actual file\n",
    "X = df.drop(\"species\", axis=1).values\n",
    "y = LabelEncoder().fit_transform(df[\"species\"])\n",
    "# Assume the last column is the target\n",
    "# X = df.iloc[:, :-1]  # Features\n",
    "# y = df.iloc[:, -1]   # Target\n",
    "\n",
    "# Preprocess\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Define model\n",
    "class IrisClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = IrisClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=reg_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save model\n",
    "# torch.save(model.state_dict(), \"iris_model.pt\")\n",
    "torch.save(model.state_dict(), model_output)\n",
    "\n",
    "# Evaluate\n",
    "with torch.no_grad():\n",
    "    preds = torch.argmax(model(X_test), dim=1)\n",
    "    acc = (preds == y_test).float().mean()\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02fdc4e",
   "metadata": {},
   "source": [
    "Step 15 - Run PyTorch Framework IrisClassifier model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3ac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/pytorch_iris.py --training_data ./data/iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797952f9",
   "metadata": {},
   "source": [
    "Step 16 - Create Tensoflow Framework Iris Keras Sequential model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b4495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./src/tensorflow_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/tensorflow_iris.py\n",
    "# tensorflow_iris.py\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# get parameters\n",
    "parser = argparse.ArgumentParser(\"train\")\n",
    "parser.add_argument(\"--training_data\", type=str, default='./iris-data/iris.csv', help=\"Path to training data\")\n",
    "parser.add_argument(\"--reg_rate\", type=float, default=0.01)\n",
    "parser.add_argument(\"--model_output\", type=str, default='./model/iris_tf.keras', help=\"Path of output model, file extension .keras instead of .h5 required\")\n",
    "parser.add_argument(\"--test_size\", type=float, default=0.30, help=\"test size\")\n",
    "parser.add_argument(\"--random_state\", type=int, default=0, help=\"random state\")\n",
    "parser.add_argument(\"--n_epoch\", type=int, default=50, help=\"number of epochs\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=0.01, help=\"learning rate\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"batch size\")\n",
    "parser.add_argument(\"--gpu\", type=str, default=\"no\", help=\"if gpu available yes/no\")\n",
    "args = parser.parse_args()\n",
    "print(args)\n",
    "\n",
    "training_data = args.training_data\n",
    "reg_rate = args.reg_rate\n",
    "model_output = args.model_output\n",
    "test_size = args.test_size\n",
    "random_state = args.random_state\n",
    "n_epoch = args.n_epoch\n",
    "learning_rate = args.learning_rate\n",
    "batch_size = args.batch_size\n",
    "gpu = args.gpu\n",
    "print(f\"GPU enabled: {gpu}\")\n",
    "\n",
    "# Load Iris dataset\n",
    "df = pd.read_csv(training_data)  # Replace with your actual file\n",
    "X = df.drop(\"species\", axis=1).values\n",
    "y = LabelEncoder().fit_transform(df[\"species\"])\n",
    "\n",
    "# Preprocess\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(batch_size, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# Train with GPU if available\n",
    "if gpu.lower() == \"yes\":    \n",
    "    print(\"Training with GPU...\")\n",
    "    with tf.device('/GPU:0'):\n",
    "        model.fit(X_train, y_train, epochs=n_epoch, batch_size=batch_size, verbose=1)\n",
    "else:\n",
    "    print(\"Training with CPU...\")   \n",
    "    model.fit(X_train, y_train, epochs=n_epoch, batch_size=batch_size, verbose=1)\n",
    "\n",
    "# Save model\n",
    "model.save(model_output) # Evaluate\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1969db2",
   "metadata": {},
   "source": [
    "Step 17 - Run Tensorflow Framework Iris Keras Sequential model script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40faf078",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./src/tensorflow_iris.py --training_data ./data/iris.csv --gpu yes --n_epoch 80 --learning_rate 0.01 --batch_size 16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
